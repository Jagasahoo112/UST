
SAMPLE REQUEST

{
    "originalFileId":"6dd18ee7965f4fc8a987ba4c3e7e45ff" , 
    "targetFileId":"1d4ec56ed00540ecb64de9d863ff76d9"
}


SAMPLE RESPONSE:

{
    "error": null,
    "response": {
        "similarity": [
            {
                "education": {
                    "similarity_score": 100.0,
                    "source": "Educational Qualifications: C ourse B. Tech (Electronics and Communication) Intermediate High School Institution Department of Electronics and communication, University of Allahabad Zakir Husain Model Sr. Sec. School St. Fidelis Sr. Sec. School Board/University/Medium University of Allahabad C.B.S.E C.B.S.E year 2014 2009 2007 Percentage 74.21 72 81",
                    "target": "Educational Qualifications: C ourse B. Tech (Electronics and Communication) Intermediate High School Institution Department of Electronics and communication, University of Allahabad Zakir Husain Model Sr. Sec. School St. Fidelis Sr. Sec. School Board/University/Medium University of Allahabad C.B.S.E C.B.S.E year 2014 2009 2007 Percentage 74.21 72 81"
                },
                "experience": {
                    "similarity_score": 100.0,
                    "source": "'les: Developer Responsibilities: Developing Generic Script in unix to load data from files to staging table. Created procedure to validate the data in our system against the one provided by wholesaler. Converting the file from excel to csv using MQ process. Automation of job in Workload Automation Tool to trigger the job when the file is placed on Linux server. Performing Unit testing, UAT and End-User training for enhancement and new requirements by users Conducted end user training sessions for system users Project Details Project: DXP Customer - Product Company: IRIS Software Duration: Aug' 19 - Till Now Client: LifeCare Overview of Project: In DXP project, we have different models like Recommendation System, Topic Modelling, Churn Prediction, Predictive Model and Resource Optimization for defining customer, clients and products relation. It help the business to find different metrics and help them to make important decisions. The data is extracted from oracle and MySQL database and dump in Greenplum. The data is used from Greenplum warehouse and processed (cleaning and translation) for giving as input to Model. Roles: Data Scientist Responsibilities: Designing flow of data from source to warehouse and pre-processing, feature engineering and model creation. Data visualization (box-plot, density plot, scatter plot, heat maps) to get the insights of data using python libraries like Seaborn and Matplotlib. Exploratory data analysis, data cleaning, standardization, data imputation and merging of multiple data sets Feature Engineering and Model Creation (different models Linear Regression  SVM, Decision Tree). Built Random Forest Classifier, Logistic Regression, XGB Classifier, Naive- Baye's classifier, K Means clustering, LDA model, neural network. deep learning model, keras model. Created recommendation engine based upon collaborative filtering and popularity based approaches. Deployment of machine learning models as pickle files and an API using Python Flask framework. Developed code to extract tabular data from PDFs and used NLP to extract all relevant entities from paragraphs. Implementation of NLP techniques which includes lemmatization, n-gram creation, stop words removal. Created topics using K Means Clustering on top of word2vec model and using LDA on top of Bag-of-words and TFIDF vectors Training model with different data set and using different model parameters to optimise the model. Creating design document, test plan, test scripts and store everything on github for deployment in production. Attending daily scrum/Kanban meeting for the daily task update on the current sprint Maintain/Keep the track of the Automated Smoke/ Regression Suite for diff. branches and resolve/fix issues if reported. Project: EDM LCR Company: IRIS Software Duration: Jul' 18 - Aug' 19 Client: LifeCare Overview of Project: EDM LCR is a lifecare reporting for different categories. Development on different tools in Tableau and python for data visualization to predict important business drivers with complete data processing. Roles: Data Analyst                 Responsibilities: Designing of process and flow of data. Built a tool in Tableau to predict important business drivers which helps in improving Customer Experience. Developed and Analyzed the survey to find out improvement areas in the already on-going project. ETL process creation for extracting, transformation and loading data in Greenplum. Creation of unix scripts for Jobs and procedure for enhancing the functionality of different applications. Creating generic Python scripts which can be used in different application. Automating processes with different control parameters. Development and update Tableau reports. Project: Global Data Warehouse : POLARIS Company: TCS Duration: Aug' 15 - Dec' 15 Client: A large US Based Pharmaceuticals company Overview of Project: CBRecon Apps-> To reconciled the data from wholesaler and customer fo the discount avail,. Project: Capital ME Consolidation & Functional Support Company: TCS Duration: Nov' 14 - Aug' 15 Client: General Electric - Capital Overview of Project: Aim of the project was to consolidate data to Oracle GL from all the legacy or external systems. Roles: Developer       Responsibilities: Created Interface Engine for file processing and control files for loading data sheets into the oracle tables. Created functions and procedures for new the legal entities in PL/SQL. Documentation for new legal entities according to client requirement for technical design of project module. Project: Global Data Warehouse : POLARIS Company: TCS Duration: Dec' 15 - Jul' 18 Client: A large US Based Pharmaceuticals company Overview of Project: POLARIS is a global data warehouse containing data from US and other affiliates. The project gamut of tools/applications involved in the project includes LINUX, Aginity, ETL (NZSQL), Database: Oracle and Netezza databases. Roles: Developer/Offshore Tech Lead           Responsibilities: Working as a DevOps Offshore Team lead for POLARIS ETL tasks with procedures, functions and scripts. Automate the manual entry of data in tables and make scripts and procedure to load them in required tables Creating design document, test plan, test scripts and store everything on github for deployment in production Development and debugging SQL codes and cubes. Development and update python codes.",
                    "target": "'les: Developer Responsibilities: Developing Generic Script in unix to load data from files to staging table. Created procedure to validate the data in our system against the one provided by wholesaler. Converting the file from excel to csv using MQ process. Automation of job in Workload Automation Tool to trigger the job when the file is placed on Linux server. Performing Unit testing, UAT and End-User training for enhancement and new requirements by users Conducted end user training sessions for system users Project Details Project: DXP Customer - Product Company: IRIS Software Duration: Aug' 19 - Till Now Client: LifeCare Overview of Project: In DXP project, we have different models like Recommendation System, Topic Modelling, Churn Prediction, Predictive Model and Resource Optimization for defining customer, clients and products relation. It help the business to find different metrics and help them to make important decisions. The data is extracted from oracle and MySQL database and dump in Greenplum. The data is used from Greenplum warehouse and processed (cleaning and translation) for giving as input to Model. Roles: Data Scientist Responsibilities: Designing flow of data from source to warehouse and pre-processing, feature engineering and model creation. Data visualization (box-plot, density plot, scatter plot, heat maps) to get the insights of data using python libraries like Seaborn and Matplotlib. Exploratory data analysis, data cleaning, standardization, data imputation and merging of multiple data sets Feature Engineering and Model Creation (different models Linear Regression  SVM, Decision Tree). Built Random Forest Classifier, Logistic Regression, XGB Classifier, Naive- Baye's classifier, K Means clustering, LDA model, neural network. deep learning model, keras model. Created recommendation engine based upon collaborative filtering and popularity based approaches. Deployment of machine learning models as pickle files and an API using Python Flask framework. Developed code to extract tabular data from PDFs and used NLP to extract all relevant entities from paragraphs. Implementation of NLP techniques which includes lemmatization, n-gram creation, stop words removal. Created topics using K Means Clustering on top of word2vec model and using LDA on top of Bag-of-words and TFIDF vectors Training model with different data set and using different model parameters to optimise the model. Creating design document, test plan, test scripts and store everything on github for deployment in production. Attending daily scrum/Kanban meeting for the daily task update on the current sprint Maintain/Keep the track of the Automated Smoke/ Regression Suite for diff. branches and resolve/fix issues if reported. Project: EDM LCR Company: IRIS Software Duration: Jul' 18 - Aug' 19 Client: LifeCare Overview of Project: EDM LCR is a lifecare reporting for different categories. Development on different tools in Tableau and python for data visualization to predict important business drivers with complete data processing. Roles: Data Analyst                 Responsibilities: Designing of process and flow of data. Built a tool in Tableau to predict important business drivers which helps in improving Customer Experience. Developed and Analyzed the survey to find out improvement areas in the already on-going project. ETL process creation for extracting, transformation and loading data in Greenplum. Creation of unix scripts for Jobs and procedure for enhancing the functionality of different applications. Creating generic Python scripts which can be used in different application. Automating processes with different control parameters. Development and update Tableau reports. Project: Global Data Warehouse : POLARIS Company: TCS Duration: Aug' 15 - Dec' 15 Client: A large US Based Pharmaceuticals company Overview of Project: CBRecon Apps-> To reconciled the data from wholesaler and customer fo the discount avail,. Project: Capital ME Consolidation & Functional Support Company: TCS Duration: Nov' 14 - Aug' 15 Client: General Electric - Capital Overview of Project: Aim of the project was to consolidate data to Oracle GL from all the legacy or external systems. Roles: Developer       Responsibilities: Created Interface Engine for file processing and control files for loading data sheets into the oracle tables. Created functions and procedures for new the legal entities in PL/SQL. Documentation for new legal entities according to client requirement for technical design of project module. Project: Global Data Warehouse : POLARIS Company: TCS Duration: Dec' 15 - Jul' 18 Client: A large US Based Pharmaceuticals company Overview of Project: POLARIS is a global data warehouse containing data from US and other affiliates. The project gamut of tools/applications involved in the project includes LINUX, Aginity, ETL (NZSQL), Database: Oracle and Netezza databases. Roles: Developer/Offshore Tech Lead           Responsibilities: Working as a DevOps Offshore Team lead for POLARIS ETL tasks with procedures, functions and scripts. Automate the manual entry of data in tables and make scripts and procedure to load them in required tables Creating design document, test plan, test scripts and store everything on github for deployment in production Development and debugging SQL codes and cubes. Development and update python codes."
                },
                "name": {
                    "similarity_score": 6.0,
                    "source": "Signh Shashank",
                    "target": "Kumar Neela"
                },
                "skills": {
                    "similarity_score": 100.0,
                    "source": " \n \n  \n Known: Hindi  English \n Hobbies and Interests: Solving puzzles  visiting places  dancing  watching movies  acting and singing. \n Technical Skills Key skill Module Algorithms Data Warehouse/ Database Platform, Tools & Suite Languages Version Control Data Science, Machine Learning, Deep Learning, Statistical Modelling, Predictive Modelling, Feature Engineering, Data Wrangling, Data Mining, Data Cleaning, Data Visualization, Natural Language Processing (NLP), LSTM, Recommendation system, Al, Bigdata Analysis, Hadoop, word2Vec, Python Flask API, Tableau, Oracle SQL, PL/SQL, PostgresSQL, Unix, NZQL, Netezza, Greenplum. Numpy, Pandas, Matplotlib, Scipy, Sklearn, Keras, Spacy, nltk Linear Regression, Logistic Regression, SVM, Decision Tree, PCA, Neural Network, CNN, Random Forest Classifier, Naive- Baye's classifier, K Means clustering, Recommendation System using Collaborative Filtering Greenplum, MYSQL, Oracle 19C/11g, Netezza/IBM Puredata for Analytics Windows, UNIX, MS Office, Anaconda - Jupyter Notebook, Python IDLE, Putty, AquaData Studio, WA application monitoring tool, Data Loader, ADI, Winscp, filezilla, pspad Python, SQL-PL/SQL, Shell Scripting GIT",
                    "target": " \n \n  \n Known: Hindi  English \n Hobbies and Interests: Solving puzzles  visiting places  dancing  watching movies  acting and singing. \n Technical Skills Key skill Module Algorithms Data Warehouse/ Database Platform, Tools & Suite Languages Version Control Data Science, Machine Learning, Deep Learning, Statistical Modelling, Predictive Modelling, Feature Engineering, Data Wrangling, Data Mining, Data Cleaning, Data Visualization, Natural Language Processing (NLP), LSTM, Recommendation system, Al, Bigdata Analysis, Hadoop, word2Vec, Python Flask API, Tableau, Oracle SQL, PL/SQL, PostgresSQL, Unix, NZQL, Netezza, Greenplum. Numpy, Pandas, Matplotlib, Scipy, Sklearn, Keras, Spacy, nltk Linear Regression, Logistic Regression, SVM, Decision Tree, PCA, Neural Network, CNN, Random Forest Classifier, Naive- Baye's classifier, K Means clustering, Recommendation System using Collaborative Filtering Greenplum, MYSQL, Oracle 19C/11g, Netezza/IBM Puredata for Analytics Windows, UNIX, MS Office, Anaconda - Jupyter Notebook, Python IDLE, Putty, AquaData Studio, WA application monitoring tool, Data Loader, ADI, Winscp, filezilla, pspad Python, SQL-PL/SQL, Shell Scripting GIT"
                }
            }
        ],
        "word_cloud": {
            "11g": 0.14285714285714285,
            "19C": 0.14285714285714285,
            "ADI": 0.14285714285714285,
            "API": 0.14285714285714285,
            "Al": 0.14285714285714285,
            "Algorithms": 0.14285714285714285,
            "Anaconda": 0.14285714285714285,
            "Analysis": 0.14285714285714285,
            "Analytics": 0.14285714285714285,
            "AquaData": 0.14285714285714285,
            "Baye": 0.14285714285714285,
            "Bigdata": 0.14285714285714285,
            "CNN": 0.14285714285714285,
            "Classifier": 0.2857142857142857,
            "Cleaning": 0.14285714285714285,
            "Collaborative": 0.14285714285714285,
            "Control": 0.14285714285714285,
            "Data": 1.0,
            "Database": 0.14285714285714285,
            "Decision": 0.14285714285714285,
            "Deep": 0.14285714285714285,
            "Engineering": 0.14285714285714285,
            "English": 0.14285714285714285,
            "Feature": 0.14285714285714285,
            "Filtering": 0.14285714285714285,
            "Flask": 0.14285714285714285,
            "Forest": 0.14285714285714285,
            "GIT": 0.14285714285714285,
            "Greenplum": 0.2857142857142857,
            "Hadoop": 0.14285714285714285,
            "Hindi": 0.14285714285714285,
            "Hobbies": 0.14285714285714285,
            "IBM": 0.14285714285714285,
            "IDLE": 0.14285714285714285,
            "Interests": 0.14285714285714285,
            "Jupyter": 0.14285714285714285,
            "Keras": 0.14285714285714285,
            "Key": 0.14285714285714285,
            "Known": 0.14285714285714285,
            "LSTM": 0.14285714285714285,
            "Language": 0.2857142857142857,
            "Learning": 0.2857142857142857,
            "Linear": 0.14285714285714285,
            "Loader": 0.14285714285714285,
            "Logistic": 0.14285714285714285,
            "MS": 0.14285714285714285,
            "MYSQL": 0.14285714285714285,
            "Machine": 0.14285714285714285,
            "Matplotlib": 0.14285714285714285,
            "Means": 0.14285714285714285,
            "Mining": 0.14285714285714285,
            "Modelling": 0.2857142857142857,
            "Module": 0.14285714285714285,
            "NLP": 0.14285714285714285,
            "NZQL": 0.14285714285714285,
            "Naive": 0.14285714285714285,
            "Natural": 0.14285714285714285,
            "Netezza": 0.2857142857142857,
            "Network": 0.14285714285714285,
            "Neural": 0.14285714285714285,
            "Notebook": 0.14285714285714285,
            "Numpy": 0.14285714285714285,
            "Office": 0.14285714285714285,
            "Oracle": 0.2857142857142857,
            "PCA": 0.14285714285714285,
            "PL SQL": 0.2857142857142857,
            "Pandas": 0.14285714285714285,
            "Platform": 0.14285714285714285,
            "PostgresSQL": 0.14285714285714285,
            "Predictive": 0.14285714285714285,
            "Processing": 0.14285714285714285,
            "Puredata": 0.14285714285714285,
            "Putty": 0.14285714285714285,
            "Python": 0.42857142857142855,
            "Random": 0.14285714285714285,
            "Recommendation system": 0.2857142857142857,
            "Regression": 0.2857142857142857,
            "SQL PL": 0.2857142857142857,
            "SVM": 0.14285714285714285,
            "Science": 0.14285714285714285,
            "Scipy": 0.14285714285714285,
            "Scripting": 0.14285714285714285,
            "Shell": 0.14285714285714285,
            "Sklearn": 0.14285714285714285,
            "Solving": 0.14285714285714285,
            "Spacy": 0.14285714285714285,
            "Statistical": 0.14285714285714285,
            "Studio": 0.14285714285714285,
            "Suite": 0.14285714285714285,
            "Tableau": 0.14285714285714285,
            "Technical": 0.14285714285714285,
            "Tree": 0.14285714285714285,
            "Unix": 0.2857142857142857,
            "Version": 0.14285714285714285,
            "Visualization": 0.14285714285714285,
            "WA": 0.14285714285714285,
            "Warehouse": 0.14285714285714285,
            "Windows": 0.14285714285714285,
            "Winscp": 0.14285714285714285,
            "Wrangling": 0.14285714285714285,
            "acting": 0.14285714285714285,
            "application": 0.14285714285714285,
            "clustering": 0.14285714285714285,
            "dancing": 0.14285714285714285,
            "filezilla": 0.14285714285714285,
            "monitoring": 0.14285714285714285,
            "movies": 0.14285714285714285,
            "nltk": 0.14285714285714285,
            "places": 0.14285714285714285,
            "pspad": 0.14285714285714285,
            "puzzles": 0.14285714285714285,
            "singing": 0.14285714285714285,
            "skill": 0.2857142857142857,
            "tool": 0.2857142857142857,
            "using": 0.14285714285714285,
            "visiting": 0.14285714285714285,
            "watching": 0.14285714285714285,
            "word2Vec": 0.14285714285714285
        }
    }
}